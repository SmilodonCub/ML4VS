{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Graphical Models\n",
    "\n",
    "## Notes for Qasim's Machine Learning for Vision Science class\n",
    "\n",
    "Koller, Daphne, and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009.\n",
    "\n",
    "![](https://images-na.ssl-images-amazon.com/images/I/511h90OxkJL._SX444_BO1,204,203,200_.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**declarative representation** the separation of knowledge and reasoning. model-based methods: construct a model of the system about hich we would like to reason.  \n",
    "**uncertainty** arises because of our limitations in our ability to observe the world, limitations in our ability to model it, and possible even because of innate nondeterminism (complexity).  \n",
    "**To obtain meaningful conclusions, we need to reason not just about what is possible, but also about what is probable** probabilistic models provide a more faithful approimation of reality ... allow us to sweep possible but improbable events under the 'probabilistic rug' to simplify the description of the system. This framework allows us to consider options that are unlikely, yet not impossible, without reducing our conclusions to content free lists of every possibiliy.  \n",
    "\n",
    "**probabilistic graphical models (PGM)** a mechanism for exploiting structure from complex distributions to describe the system compactly and in a way that allows them to be constructed and utilized effectively. Rather than encode the probability of every possible assignment to all the variables in the domain, PGM breaks up the representation into smaller factors over a much smaller space of possibilities.  \n",
    "\n",
    "**Bayesian Networks:** uses directed graphs  \n",
    "**Markov Networks:** uses undirected graphs  \n",
    "\n",
    "PGM Advantages:\n",
    "1. **representation:** distributions can be written down tractable & transparent which makes it easier for a human to apply domain knowledge. give a reasonable encoding of our model (our understanding of the system).\n",
    "2. **inference:** this framework allows us to test inferences about the model. The model can be used to answer questons about the system\n",
    "3. **learning:** this framework supports data-driven insights to refine the model in future iterations. use accumulated data to refine the representation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundations\n",
    "\n",
    "background material regarding key concepts from probability theory, information theory and graph theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability Distributions\n",
    "\n",
    "**probability** the degree of confidence that an event of uncertain nature will occur.  \n",
    "**events** can be decribed as having a **space** of possible outcomes, $\\Omega$ For example, the event space that describes the roll of 1 dice $\\Omega = { 1,2,3,4,5,6 }$  \n",
    "Three basic properties of event spaces:\n",
    "1. it contains the empty event $\\emptyset$ and the trivial event $\\Omega$\n",
    "2. It is closed under union ~ if two events are in the event space, then their union is also in the event space, or if $\\alpha, \\beta \\in \\mathcal{S}$, ... $\\alpha \\cup \\beta$\n",
    "3. it is closed under complementation. if $\\alpha \\in \\mathcal{S}$, ... $\\Omega - \\alpha \\in \\mathcal{S}$  \n",
    "Q: is closed under complementation another way of saying the events are independent?\n",
    "<br>\n",
    "\n",
    "The **probability distribution** $P over (\\Omega , \\mathcal{S})$ is a mapping of events in $\\mathcal{S}$ to realvalues with the following properties\n",
    "1. $P(\\alpha)  \\geq 0$ for all $\\alpha \\in \\mathcal{S}$ (probabilities are not negative)\n",
    "2. $P(\\Omega) = 1$ (sum of all probabilities = 1)\n",
    "3. If $\\alpha , \\beta \\in \\mathcal{S}$ and  $\\alpha \\cap \\beta \\neq 0$, ... $P(\\alpha \\cup \\beta) = P(\\alpha) + P(\\beta)$ (probability of two independent events occuring is the sum of each event's probability)\n",
    "\n",
    "What is a probability?\n",
    "* **frequentist:** the probability is the fraction of times the event occurs\n",
    "* **subjective degrees of belief:** the degree of belief that an event will occur\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Concepts in Probability\n",
    "\n",
    "**Conditional Probability** after learning that event $\\alpha$ occured, how do we update our estimate of the probability that event $\\beta$ will occur? The probability the $\\beta$ occurs given that we know $\\alpha$ occured is the relative proportion of outcomes satisfying $\\beta$ among $\\alpha$. This takes a distribution and returns another over the same probability space.\n",
    "\n",
    "$$P(\\beta | \\alpha) = \\frac{P(\\alpha \\cap \\beta)}{P(\\alpha)}$$\n",
    "\n",
    "from this equation we see that\n",
    "\n",
    "$$P(\\alpha \\cap \\beta) = P(\\alpha | \\beta)P(\\alpha)$$\n",
    "\n",
    "it is also the case that\n",
    "\n",
    "$$P(\\beta \\cap \\alpha) = P(\\beta | \\alpha)P(\\beta)$$\n",
    "\n",
    "and since\n",
    "\n",
    "$$P(\\alpha \\cap \\beta) = P(\\beta \\cap \\alpha)$$\n",
    "\n",
    "we can state\n",
    "\n",
    "$$P(\\alpha | \\beta)P(\\alpha) = P(\\beta | \\alpha)P(\\beta)$$\n",
    "\n",
    "which leads us to **Bayes' Rule** an immediate consequence of the definition of conditional probability\n",
    "\n",
    "$$P(\\alpha | \\beta) = \\frac{P(\\beta | \\alpha)P(\\alpha)}{P(\\beta)}$$\n",
    "\n",
    "Bayes' is great because it allows us to find inverse conditional probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of a random subject to test positive and have TB =  1.87 %\n"
     ]
    }
   ],
   "source": [
    "#Bayes' example with TB disease screaning\n",
    "\n",
    "# P(TB| positive) = P(positive | TB)P(TB) / P(positive)\n",
    "\n",
    "P_TB = 0.001 #P(TB) 1 in 1000 people who get tested are infected\n",
    "P_positive = (0.001*0.95)+(0.999*0.05) #P_positive = TruePositives + FalseAlarms\n",
    "P_positive_given_TB = 0.95 #if TB probability of positive test\n",
    "\n",
    "P_TB_given_positive = P_TB * P_positive_given_TB / P_positive\n",
    "print( 'probability of a random subject to test positive and have TB = ', round( 100* P_TB_given_positive, 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Variables and Joint Distributions\n",
    "\n",
    "**random variables** associate the values of a feature to our system  \n",
    "**categorical**, or discrete variables take one of a few values where:\n",
    "$$\\sum^{k}_{i=1} P(X = x^i ) = 1 $$\n",
    "**bernoulli distribution** describes binary random variables  \n",
    "**marginal distribution** $P(X)$ describes the distribution of events that can be described by a random variable. This is a probability distribution restricted to the subset of $\\mathcal{S}$ that can be described by a random variable $X$  \n",
    "**joint distributions** the joint distribution over a set of random variables is the distribution that assigns probabilities to events that are specified in terms of these random variables. joint probabilities have to be consistent with marginal probabilities.\n",
    "\n",
    "Joint probabilities and marginal probabilities for $\\xi \\in \\mathcal{X} (Intelligence,Grade)$\n",
    "\n",
    "| Intelligence -> |   |      |      | Grade Marg. |\n",
    "|-----------------|---|------|------|-------------|\n",
    "|                 |   | low  | high |             |\n",
    "|                 | A | 0.07 | 0.18 | 0.25        |\n",
    "| Grade           | B | 0.28 | 0.09 | 0.37        |\n",
    "|                 | C | 0.35 | 0.03 | 0.38        |\n",
    "| Int. Marginal   |   | 0.7  | 0.3  | 1           |\n",
    "\n",
    "\n",
    "The marginal distributions give the prior knowledge about a random variable (e.g. P(Intelligence)) before we know anything about other random variables (i.e. P(Grade)) whereas the joint probabilities give the distribution of a random variable given the knowledge of another randome variable (e.g. P( Intelligence = high | Grade = A ) = 0.18/0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independence\n",
    "\n",
    "An event $\\alpha$ is independent of event $\\beta$, written as $P  \\models (\\alpha \\bot \\beta)$, if$P(\\alpha | \\beta ) = P(\\alpha)$ or if $P(\\beta) = 0$  \n",
    "\n",
    "In other words, $P  \\models (\\alpha \\bot \\beta)  \\equiv P(\\alpha \\cap \\beta) = P(\\alpha)P(\\beta)$ \n",
    "\n",
    "Example: tossing a coin. if we toss the coin again, the next outcome has no dependence on the previous.  \n",
    "\n",
    "**conditional independence** we say that an event $\\alpha$ is conditionally independent of event $\\beta$ given event $\\gamma$ in $P$, written as $P \\models (\\alpha \\bot \\beta | \\gamma )$ if $P(\\alpha | \\beta \\cap \\gamma ) = P( \\alpha | \\gamma )$ or if $P(\\beta \\cap \\gamma) = 0$ In other words $P \\models (\\alpha \\bot \\beta | \\gamma ) \\equiv P(\\alpha \\cap \\beta | \\gamma) =P(\\alpha | \\gamma)P(\\beta | \\gamma)$  \n",
    "\n",
    "An independence statement over random variables is a universal quantification over all possible values of the random variables.  \n",
    "\n",
    "**independence symmetry** $( X \\bot Y | Z ) \\Longrightarrow ( Y \\bot X | Z )$  \n",
    "**decomposition** $( X \\bot Y,W | Z ) \\Longrightarrow ( X \\bot Y | Z )$  \n",
    "**Weak Union** $( X \\bot Y,W | Z ) \\Longrightarrow ( X \\bot Y | Z,W )$  \n",
    "**contraction** $( X \\bot Y | Z,Y ) \\mbox{ and } ( X \\bot Y |Z) \\Longrightarrow ( X \\bot Y,W | Z )$  \n",
    "**intersection (positive dist)** $( X \\bot Y | Z,W ) \\mbox{ and } ( X \\bot W |Z,Y) \\Longrightarrow ( X \\bot Y,W | Z )$\n",
    "\n",
    "Q: what are the consequences of these?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying a Distribution\n",
    "\n",
    "**Probability Query**\n",
    "1. The Evidence: a subset $E$ of random variables in the model, and an instantiation $e$ to these variables\n",
    "2. The Query Variables: a subset $Y$ of random variables in the network.\n",
    "3. The Task: compute the posterior probability distribution $P(Y | E=e)$\n",
    "\n",
    "**MAP Query** the most probable explanation\n",
    "find the most likely assignment to the variables in $W$ given the evidence $E=e$\n",
    "$$MAP(W|e)= argmax_w P( w,e )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Spaces\n",
    "\n",
    "**probability density function** defining probability for a continuous variable. a nonnegative integratable function such that the integral over the set of possible values of $X$ is 1:\n",
    "$$\\int\\limits_{Val(X)} p(x)dx = 1$$\n",
    "\n",
    "**uniform distributions** $p(x) = \\left\\{ \\begin{array}{rcl}\n",
    " \\frac{1}{b-a} &  b\\leq x \\leq a \\\\\n",
    "0 & \\mbox{otherwise}\n",
    "\\end{array}\\right.$\n",
    "\n",
    "**gaussian distribution** (mean = 0 var = 1)   $p(x) = \\frac{1}{ \\sqrt{ 2 \\pi \\sigma } } e^{- \\frac{ (x-\\mu )^2 }{2\\sigma ^2 } }$   \n",
    " \n",
    "**joint probability density functions** specify the probability of any joint event over the the continuous random variables of interest.  \n",
    "Let $P$ be any joint distribution over continuous random variables $X_1 \\mbox{,...,} X_n$. A function $p(x_1 \\mbox{,...,} x_n)$ is a joint probability density function of $X_1 \\mbox{,...,} X_n$ if:\n",
    "1. $p(x_1 \\mbox{,...,} x_n) \\geq 0$ for all values $x_1 \\mbox{,...,} x_n$ of $X_1 \\mbox{,...,} X_n$\n",
    "2. $p$ is an integratable function\n",
    "3. for any choice of $a_1 \\mbox{,...,} a_n$, and $b_1 \\mbox{,...,} b_n$,\n",
    "$$P(a_1 \\leq X_1 \\leq b_1 \\mbox{,...,} a_n \\leq X_n \\leq b_n = \\int\\limits_{a_1}^{b_1} \\cdots \\int\\limits_{a_n}^{b_n} p(x_1 \\mbox{,...,} x_n)dx_1 \\mbox{...} dx_n$$\n",
    "\n",
    "**conditional density functions** describe conditional distributions of continuous variables  \n",
    "$$P( Y|x) = \\lim_{\\epsilon  \\rightarrow 0}P(Y|x-\\epsilon \\leq X \\leq x+\\epsilon )$$\n",
    "when $\\epsilon$ is sufficiently small and with some approximation, we get the conditional density function of Y given X is defined as:\n",
    "$$p(y|x)= \\frac{p(x,y)}{p(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation & Variance\n",
    "\n",
    "**expectation** tells us the mean (expected) value of a random variable  \n",
    "1. discrete random variable  \n",
    "$$E_p[X]= \\sum_{x} x\\centerdot P(x )$$\n",
    "\n",
    "2. continuous random variable\n",
    "$$E_p[X] = \\int x\\centerdot p(x)dx$$\n",
    "\n",
    "The expectation is a linear function in the random variable & this property is true even when the variables are not independent.  \n",
    "The expectation of the sum of two independent random variables is the individual sums.\n",
    "$$E[X+Y]=E[X] + E[Y]$$\n",
    "If $X$ and $Y$ are independent, then the expectation of a product of two random variables is the product of their individual expectation.\n",
    "$$E[X\\centerdot Y]=E[X] + E[Y]$$\n",
    "\n",
    "**variance** tells us how far an observed value deviates from the expected value (mean). The variance is defines are the squared difference between X and the expected value; it gives us an indication of the spread of observed values.\n",
    "$$Var_P[X] = E_P[(X-E_P[X])^2] = E[X^2]-(E[X])^2$$\n",
    "\n",
    "If $X$ and $Y$ are independent, then $Var[X + Y] = Var[X] + Var[Y]$  \n",
    "variance often scales as a quadratic function of $X$, therefore it is commonplace to refer to the square root of the variance, or the **standard deviation** This is because it is improbable to encounter values farther than several standard deviations from the expected value, so taking the square root effectively normalizes for distance from the expected value.\n",
    "$$\\sigma _X = \\sqrt{Var[X]}$$\n",
    "\n",
    "For a gaussian distribution $E{X}=\\mu $ and $Var[X]=\\sigma ^2$ Therefore, the paramters of a gaussian conveniently give you the expectation and variance of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes and Edges\n",
    "\n",
    "a **graph** is a data structure $\\mathcal{K}$ consisting of a set of nodes and a set of edges.  \n",
    "nodes may be connected by a **directed edge** $X_i \\rightarrow X_j$ or an **undirected edge** $X_i - X_j $ and we assume that for each pair of nodes there exists only one type of edge.  \n",
    "a graph is said to be **directed** ($\\mathcal{G}$) if all edges are either $X_i \\rightarrow X_j$ or $X_j \\rightarrow X_i$  \n",
    "a graph is said to be **undirected** ($\\mathcal{H}$) if all edges are $X_i - X_j$  \n",
    "\n",
    "for the directed pair $X_i \\rightarrow X_j \\in \\mathcal{E}$, we say that $X_j$ is the **child** ($Ch_X$) and that $X_i$ is the **parent** ($Pa_x$), whereas for the pair $X_i - X_j \\in \\mathcal{E}$, we say that $X_i$ and $X_j$ are **neighbors** ($Nb_x$). When the nature of the edge is unknown ($X_i \\rightleftharpoons X_j$), we say that $X_i$ and $X_j$ are  adjacent.  \n",
    "the **degree** of a node refers to the number of edges and the **indegree** is the number of inward directed edges $Y \\rightarrow X$. the degree of a graph refers to the maximum degree of nodes in the network.\n",
    "\n",
    "a **subgraph** is a part of a graph that is associated with a particular subset of nodes.  \n",
    "a **complete subgraph** every two nodes are connected by an edge.  \n",
    "\n",
    "Q: I don't think I understand upward closure from the way it is described.  \n",
    "\n",
    "**trails** have some kind ($\\rightleftharpoons$) of connection between nodes.  \n",
    "**paths** are more specific. the edges are in a conguous direction or are undirected.  \n",
    "a graph is **connected** if everyone node has a trail.  \n",
    "\n",
    "In a directed graph, a node is an **ancestor** if there exists a directed path to another node(s), it's **descendents**. All nots that are not in the set considered descendents are a nodes**nondescendents**.  \n",
    "a path may be cyclic making a node it's own descendent.  \n",
    "\n",
    "**directed acyclic graphs (DAG)** are the basic graphical representation that underlies Bayesian networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.2**\n",
    "1. Show that for binary random variables $X$, $Y$ , the event-level independence $(x^0 \\bot y^0 )$ implies random variable independence $(X \\bot Y )$.\n",
    "2. Show a counterexample for nonbinary variables.\n",
    "3. Is it the case that, for a binary-valued variable $Z$, we have that $(X \\bot Y | z^0 )$ implies $(X \\bot Y | Z)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 2.2 (b,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.3**  \n",
    "Consider two events $\\alpha$ and $\\beta$ such that $P(\\alpha) = p_a$ and $P(\\beta) = p_b$ . Given only that knowledge, what is the maximum and minimum values of the probability of the events $\\alpha \\cap \\beta$ and $\\alpha \\cup \\beta$. Can you characterize the situations in which each of these extreme values occurs?\n",
    "<br><br>\n",
    "The maximum value of the probability of the event $\\alpha \\cup \\beta$ occurs when there is no overlap of the distributions. In this case $P(\\alpha \\cup \\beta) = p_a + p_b$. Whereas the minimum occurs when the distributions completely overlap. In this case $P(\\alpha \\cup \\beta) = min(p_a,p_b)$.  \n",
    "The maximum value of the probability of the event $\\alpha \\cap \\beta$ occurs when there is complete overlap of the probability distributions, in which case the $P(\\alpha \\cup \\beta) = min(p_a,p_b)$. The minimum value of $\\alpha \\cap \\beta$ occurs when there is no overlap of the probability distributions, in which case $P(\\alpha \\cap \\beta) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.5**  \n",
    "Let $X,Y,Z$ be three disjoint subsets of variables such that $X = X \\cup Y \\cup Z$. Prove that $P \\models (X \\bot \n",
    "Y | Z)$ if and only if we can write P in the form:\n",
    "$P(X) = \\phi _1 (X, Z)\\phi _2 (Y , Z)$.\n",
    "\n",
    "we want to show this:\n",
    "$$P_{X,Y|Z}(x,y|z) = P_{X|Z}(x|z)P_{Y|Z}(y|z)= \\phi _1 (X, Z)\\phi _2 (Y , Z)$$\n",
    "\n",
    "start by looking at the def of conditional density function:\n",
    "$$P_{X,Y|Z}(x,y|z) = P_{X,Y,Z}(x,y,z)/P_Z(z)$$\n",
    "can be rearrange to the form:\n",
    "$$P_{X,Y,Z}(x,y,z) = P_Z(z)P_{X,Y|Z}(x,y|z)$$\n",
    "<br>\n",
    "$$=P_Z(z)P_{X|Z}(x|z)P_{Y|Z}(y|z)$$\n",
    "rearrange....\n",
    "$$=P_{X|Z}(x|z)P_{Y|Z}(y|z)P_Z(z)$$\n",
    "<br>\n",
    "now let $\\phi _1(X,Z) = P_{X|Z}(x|z)$ and then we use the chain rule(?) to let $\\phi _2(Y,Z) = P_{Y|Z}(y|z)P_Z(z)$\n",
    "<br>\n",
    "$$= \\phi _1 (X, Z)\\phi _2 (Y , Z)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
